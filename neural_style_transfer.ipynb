{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Install the required packages\n",
    "for p in (\"Knet\",\"ArgParse\",\"Images\")\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end\n",
    "include(Pkg.dir(\"Knet\",\"data\",\"imagenet.jl\"))  #imagenet.jl includes matconvnet function etc.\n",
    "\n",
    "using Knet, Images, FileIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "postprocess (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function img_normalize!(img, model_mean)\n",
    "    for i in 1:size(img, 3)\n",
    "        img[:,:,i] = (img[:,:,i] .- model_mean[i])\n",
    "    end\n",
    "    return img\n",
    "end\n",
    "\n",
    "function preprocess(img; new_size=256)\n",
    "    height = size(img,1); width = size(img,2);\n",
    "    if min(height, width) == new_size\n",
    "        img2 = img\n",
    "    elseif height <= width\n",
    "    #Resize the image\n",
    "        img2 = imresize(img, new_size, Int(round(new_size*width/height)))\n",
    "    else\n",
    "        img2 = imresize(img, Int(round(new_size*height/width)), new_size)\n",
    "    end\n",
    "    #convert the image to 3D Tensor\n",
    "    img2 = channelview(img2);\n",
    "    img2 = permutedims(img2, [3,2,1])    #result is a new array\n",
    "    img2 = Array{Float64}(img2);\n",
    "    #Normalize the image tensor by subtracting the mean and dividing with the standard dev. for each channel\n",
    "    CNN_MEAN = Array{Float64}(averageImage./255)\n",
    "    img_normalize!(img2, CNN_MEAN)   \n",
    "    #put extra dimension at the end of the tensor. size becomes [H,W,C,1]\n",
    "    img3 = reshape(img2, (size(img2)..., 1))  \n",
    "    img3 = permutedims(img3, [2,1,3,4])\n",
    "end\n",
    "\n",
    "function postprocess(img)\n",
    "    img = reshape(img, (size(img)...)[1:end-1])\n",
    "    CNN_MEAN = Array{Float64}(averageImage./255)\n",
    "    img_normalize!(img, -1*CNN_MEAN)\n",
    "    clamp!(img, 0,1)\n",
    "    img = Array{FixedPointNumbers.Normed{UInt8,8}}(img)\n",
    "    img2 = colorview(RGB, permutedims(img, [3,1,2]))\n",
    "    return img2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "display_output (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function display_output(img)\n",
    "    output = postprocess(cpu_type(copy(img)))\n",
    "    return output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Knet.KnetArray{Float64,N} where N"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the type functions\n",
    "cpu_type = Array{Float64}\n",
    "if gpu()>-1\n",
    "    dtype = KnetArray{Float64}   #use KnetArray for GPU support, if there is a connected GPU\n",
    "else\n",
    "    dtype = Array{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mLoading imagenet-vgg-verydeep-16.mat...\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "#Load the weights of the pre-trained VGG16 network\n",
    "# This procedure makes pretrained MatConvNet VGG parameters available for Knet\n",
    "function get_params(CNN, atype; last_layer=\"pool5\")\n",
    "    layers = CNN[\"layers\"]\n",
    "    weights, operations, derivatives = [], [], []\n",
    "    for l in layers\n",
    "        get_layer_type(x) = startswith(l[\"name\"], x)\n",
    "        last_layer != nothing && get_layer_type(last_layer) && break\n",
    "        operation = filter(x -> get_layer_type(x), LAYER_TYPES)[1]\n",
    "        push!(operations, operation)\n",
    "        push!(derivatives, haskey(l, \"weights\") && length(l[\"weights\"]) != 0)\n",
    "        if derivatives[end]\n",
    "            w = copy(l[\"weights\"])\n",
    "            if operation == \"conv\"\n",
    "                w[2] = reshape(w[2], (1,1,length(w[2]),1))\n",
    "            elseif operation == \"fc\"\n",
    "                w[1] = transpose(mat(w[1]))\n",
    "            end\n",
    "            push!(weights, w)\n",
    "        end\n",
    "    end\n",
    "    map(w -> map(wi->convert(atype,wi), w), weights), operations, derivatives\n",
    "end\n",
    "\n",
    "# get convolutional network to compute the outputs of five layers(conv1,relu1,conv2,relu2...,conv5,relu5)\n",
    "function get_convnet(weights, operations, derivatives)\n",
    "    function convnet(xs)     #xs is the input image to the network\n",
    "        outputs = []\n",
    "        i, j = 1, 1\n",
    "        num_weights, num_operations = length(weights), length(operations)\n",
    "        while i <= num_operations && j <= num_weights\n",
    "            if derivatives[i]\n",
    "                xs = forw(xs, operations[i], weights[j])\n",
    "                j += 1\n",
    "            else\n",
    "                xs = forw(xs, operations[i])\n",
    "            end\n",
    "            if operations[i] in (\"conv\", \"relu\")\n",
    "                push!(outputs, xs)\n",
    "            end\n",
    "            i += 1\n",
    "        end\n",
    "        return outputs\n",
    "    end\n",
    "end\n",
    "\n",
    "# convolutional network operations\n",
    "convx(x,w) = conv4(w[1], x; padding=1, mode=1) .+ w[2]\n",
    "if VERSION >= v\"0.6.0\"\n",
    "    relux(x) = relu.(x)\n",
    "else\n",
    "    relux(x) = relu(x)\n",
    "end\n",
    "poolx = pool   #max pooling\n",
    "#poolx(x) = pool(x, mode=1)  #avg pooling\n",
    "probx(x) = x\n",
    "fcx(x,w) = w[1] * mat(x) .+ w[2]\n",
    "tofunc(op) = eval(parse(string(op, \"x\")))\n",
    "forw(x,op) = tofunc(op)(x)\n",
    "forw(x,op,w) = tofunc(op)(x,w)\n",
    "\n",
    "const LAYER_TYPES = [\"conv\", \"relu\", \"pool\", \"fc\", \"prob\"] \n",
    "global _vggcache\n",
    "VGG_model = \"imagenet-vgg-verydeep-16\"\n",
    "if !isdefined(:_vggcache); _vggcache=Dict(); end\n",
    "if !haskey(_vggcache, VGG_model)\n",
    "    #Load the pre-trained VGG-16 model\n",
    "    #const vggurl = \"http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-16.mat\"\n",
    "    vgg = matconvnet(VGG_model)\n",
    "    params = get_params(vgg, dtype)\n",
    "    convnet = get_convnet(params...)\n",
    "    averageImage = reshape(convert(Array{Float64},vgg[\"meta\"][\"normalization\"][\"averageImage\"]), (1,3))\n",
    "    _vggcache[VGG_model] = vgg, params, convnet, averageImage;\n",
    "else\n",
    "    vgg, params, convnet, averageImage = _vggcache[VGG_model];\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gram_matrix (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function content_loss(content_weight, content_current, content_original)\n",
    "    #=\n",
    "    inputs:\n",
    "    compute the content loss from the content image and generated image\n",
    "    content_weight: weight of the content_loss in the total loss function\n",
    "    content_current: features of the current image. shape is (H_l, W_l, C_l, 1)\n",
    "    content_target: features of the content image. shape is (H_l, W_l, C_l ,1)\n",
    "    returning:\n",
    "    content_loss: a scalar loss value\n",
    "    =#\n",
    "    Hl, Wl, Cl, Q = size(content_current)\n",
    "    content_losses = content_weight * sum((content_current - content_original).^2) /  (4*Hl*Wl* Cl)\n",
    "    return content_losses\n",
    "end\n",
    "\n",
    "function gram_matrix(features, normalize=true)\n",
    "    #=\n",
    "    compute the Gram matrix for one layer of network from features\n",
    "    features:input image features. shape (H, W, C, N) where N is 1.\n",
    "    normalize: if true, normalize the Gram matrix by dividing by the number of parameters (H*W*C)\n",
    "    gram_mat: Gram matrix for the input image features. shape (C, C)\n",
    "    =#\n",
    "    H, W, C, N = size(features)\n",
    "    feat_reshaped = reshape(features, (H*W, C))  \n",
    "    gram_mat = transpose(feat_reshaped) * feat_reshaped  #shape:(C,C)\n",
    "    if normalize\n",
    "        return gram_mat ./ (2*H*W*C)\n",
    "    else\n",
    "        return gram_mat\n",
    "    end\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'style_loss :: NTuple{4,Any}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "style_loss"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute the style loss at a given set of layers\n",
    "Inputs:\n",
    " - feats: List of features obtained at specifed layers for the current image, as obtained by get_convnet function.\n",
    " - style_layers: List of layer indices from which the style will be extracted\n",
    " - style_targets: List of Gram matrices computed from the features yielded by the specified style layers\n",
    " - style_weights: List of weights for the style layers\n",
    "Returning:\n",
    " - style_loss: The scalar style loss obtained by taking the weighted sum of style losses in specified layers\n",
    "\"\"\"\n",
    "function style_loss(feats, style_layers, style_targets, style_weights)\n",
    "    style_losses = Float64(0.0)\n",
    "    for i in 1:length(style_layers)\n",
    "        gram_mat = gram_matrix(feats[style_layers[i]])\n",
    "        style_losses = style_losses + style_weights[i]*sum((gram_mat - style_targets[i]).^2)\n",
    "    end\n",
    "    return style_losses\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tv_loss"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute total variation loss.\n",
    "    \n",
    "Inputs:\n",
    "- img: input image, size (H, W, 3, 1)\n",
    "- tv_weight: weight of the TV loss in the overall loss equation\n",
    "    \n",
    "Returning:\n",
    "- losses: total variation loss multiplied by its weight in the overall loss equation\n",
    "\"\"\"\n",
    "\n",
    "#Total-variation regularization\n",
    "function tv_loss(img, tv_weight)\n",
    "    img = Array(img)\n",
    "    Hl, Wl, Cl, Q = size(img)\n",
    "    ver_comp = sum((img[2:end, :, :, :] - img[1:end-1, :, :, :]).^2)\n",
    "    hor_comp = sum((img[:, 2:end, :, :] - img[:, 1:end-1, :, :]).^2)\n",
    "    losses = tv_weight .* (ver_comp + hor_comp) /  (4*Hl*Wl* Cl)\n",
    "    return losses\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overall loss function that we want to optimize, by updating img_var\n",
    "function loss(img_var, content_weight,content_layer,content_target, style_layers, style_targets, style_weights, tv_weight)\n",
    "    feats = convnet(img_var)\n",
    "    # Compute loss    \n",
    "    c_loss = content_loss(content_weight, feats[content_layer], content_target)\n",
    "    s_loss = style_loss(feats, style_layers, style_targets, style_weights)\n",
    "    t_loss = tv_loss(img_var, tv_weight)\n",
    "    total_loss = c_loss + s_loss + t_loss\n",
    "    return total_loss\n",
    "end\n",
    "\n",
    "loss_gradient = gradloss(loss)  #autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "style_transfer (generic function with 2 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    "    Run the neural style transfer algorithm.\n",
    "\n",
    "    Inputs:\n",
    "    - content_image: filename of content image\n",
    "    - style_image: filename of style image\n",
    "    - image_size: size of smallest image dimension (used for content loss and generated image)\n",
    "    - style_size: size of smallest style image dimension\n",
    "    - content_layer: layer to compute the content representation\n",
    "    - content_weight: weight of the content loss\n",
    "    - style_layers: layers to compute the style representations\n",
    "    - style_weights: weights of style layers in the loss function\n",
    "    - tv_weight: weight of the total variation loss, which is used for smoothing\n",
    "    - init_random: if true, initialize to random noise\n",
    "    \"\"\"\n",
    "iterations = 500\n",
    "\n",
    "function style_transfer(content_image, style_image, image_size, style_size, content_layer, content_weight,\n",
    "                   style_layers, style_weights, tv_weight, init_random = false)\n",
    "    # Extract features for the content image\n",
    "    content_img = preprocess(load(content_image), new_size=image_size)\n",
    "    content_img_var = dtype(content_img)\n",
    "    feats = convnet(content_img_var)\n",
    "    content_target = copy(feats[content_layer])\n",
    "    \n",
    "    # Extract features for the style image\n",
    "    style_img = preprocess(load(style_image), new_size=style_size)\n",
    "    style_img_var = dtype(style_img)\n",
    "    feats = convnet(style_img_var)\n",
    "    style_targets = []\n",
    "    for i in style_layers\n",
    "        push!(style_targets, gram_matrix(copy(feats[i])))\n",
    "    end\n",
    "    # Note that we are optimizing the pixel values of the image\n",
    "    # Initialize output image to content image or noise\n",
    "    if init_random\n",
    "        #img_var = dtype(rand(size(content_img)))   #uniform r.v. between 0 and 1\n",
    "        img_var = dtype(randn(size(content_img)))  #gaussian noise (mean:0, var:1)\n",
    "    else\n",
    "        img_var = dtype(copy(content_img))\n",
    "    end\n",
    "    \n",
    "    # Set up optimization hyperparameters\n",
    "    initial_lr = 0.03   #0.03 is good for size:256.and 0.01 is good for 512.    \n",
    "    BETA_1=0.9\n",
    "    BETA_2=0.999\n",
    "    EPS=1e-08\n",
    "    #the img_var will be updated using Adam\n",
    "    optim = optimizers(img_var, Adam; lr=initial_lr, beta1=BETA_1, beta2=BETA_2, eps=EPS)\n",
    "    \n",
    "    #TRAINING: TAKE DERIVATIVE W.R.T. img_var and update img_var\n",
    "    #initialize the loss_vector so that we can plot the loss in each iteration later\n",
    "    loss_vector = []\n",
    "    info(\"Training...\")\n",
    "    for t in 1:iterations\n",
    "        grads, loss_value = loss_gradient(img_var, content_weight,content_layer,content_target, \n",
    "            style_layers, style_targets, style_weights, tv_weight)\n",
    "        \n",
    "        update!(img_var, grads, optim)\n",
    "        \n",
    "        #report errors in every five iterations\n",
    "        if t % 50 == 0\n",
    "                println((:iteration, t, :loss, loss_value))\n",
    "        end\n",
    "        push!(loss_vector, loss_value)\n",
    "    end\n",
    "    return (img_var, loss_vector);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mTraining...\n",
      "\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:iteration, 50, :loss, 73.59833058720727)\n",
      "(:iteration, 100, :loss, 46.52738002501063)\n",
      "(:iteration, 150, :loss, 39.923674043932095)\n",
      "(:iteration, 200, :loss, 37.72926122563867)\n",
      "(:iteration, 250, :loss, 35.96107320403277)\n",
      "(:iteration, 300, :loss, 34.01885477314185)\n",
      "(:iteration, 350, :loss, 33.83385265584981)\n",
      "(:iteration, 400, :loss, 32.20936562011938)\n",
      "(:iteration, 450, :loss, 31.939766903073576)\n",
      "(:iteration, 500, :loss, 63.1888698539278)\n",
      "100.925339 seconds (3.36 M allocations: 19.701 GiB, 1.46% gc time)\n"
     ]
    }
   ],
   "source": [
    "#Demo-1. Style transfer.\n",
    "content_img_def = \"content_images/sculpture1.jpg\"\n",
    "style_img_def = \"style_images/style3_buddha.jpg\"\n",
    "iterations = 500\n",
    "image_size_default = style_size_default = 256\n",
    "content_lay_default = 18    #relu4_2\n",
    "style_lay_default = (3, 7, 12, 17, 23);  #conv1_2, conv2_2, conv3_2, conv4_2, conv5_2\n",
    "\n",
    "content_weights_default = 10*Float64(1.0)\n",
    "style_weights_default = 10*Array{Float64}([0.2, 2*0.2, 3*0.2, 4*0.2, 5*0.2])\n",
    "tv_weight_def = 8*Float64(1.0);\n",
    "@time (img1, loss_vector1) = style_transfer(content_img_def, style_img_def, image_size_default, style_size_default, content_lay_default, \n",
    "    content_weights_default, style_lay_default, style_weights_default, tv_weight_def);\n",
    "save(\"output_image.jpg\", output1)\n",
    "output1 = display_output(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
