{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "This notebook implements deep CNN based image style transfer algorithm from [\"Image Style Transfer Using Convolutional Neural Networks\" (Gatys et al., CVPR 2016)](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780634).\n",
    "\n",
    "The proposed technique takes two images as inputs, i.e. a content image (generally a photograph) and a style image (generally an artwork painting). Then, it produces an output image such that the content(objects in the image) resembles the \"content image\" whereas the style i.e. the texture is similar to the \"style image\". In order words, it re-draws the \"content image\" using the artistic style of the \"style image\".\n",
    "\n",
    "Matching the style and content of the generated image with the input images is achieved by minimizing a loss function that is based on the feature representations of the images. The feature representations are computed using the representations obtained from different layers of a pre-trained VGG_16 network as suggested in the paper. Note that this network is only used for feature extraction and not trained further at all. First, the new image is initialized as Gaussian (White) Noise matrix whose shape is the same as the content image. Then, it is passed through the VGG network and its feature representations are recorded at the end of various layers of the networ. Similarly, the content and style images are also passed through the network and their representations are stored. Then, content loss is computed using the features of the content image and generated image, whereas style loss is computed using the features of style image and generated image. At this stage, the gradient of the loss function with respect to the generated image matrix is calculated. The gradients are used to optimize the pixel values of the generated image by using a back-propagation based optimization technique. The author uses L-BFGS, but this notebook uses Adam optimizer and yields nice results as well. \n",
    "\n",
    "The most important part of the presented technique is that the network weights are kept fixed and the loss function is optimized by performing gradient descent based algorithm (Adam) on the pixels of the generated image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Helper Functions for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Install the required packages if they haven't already installed\n",
    "for p in (\"Knet\",\"Images\",\"FileIO\")\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end\n",
    "include(Pkg.dir(\"Knet\",\"data\",\"imagenet.jl\"))  #imagenet.jl includes matconvnet function etc.\n",
    "\n",
    "using Knet, Images, FileIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "img_normalize!"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Normalizes the input image by subtracting the mean of the network\n",
    "    Inputs:\n",
    "    -img: an image tensor having shape of [H,W,C]\n",
    "    -model_mean: the mean of the images on which the network(VGG16) has been trained.\n",
    "    \n",
    "    Returns:\n",
    "    -img: normalized version of the input\n",
    "\"\"\"\n",
    "function img_normalize!(img, model_mean)\n",
    "    for i in 1:size(img, 3)\n",
    "        img[:,:,i] = (img[:,:,i] .- model_mean[i])\n",
    "    end\n",
    "    return img\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    "    Preprocess the input image before feeding it to the network\n",
    "    Inputs:\n",
    "    -img: an image object(generally loaded from the memory)\n",
    "    -new_size: the size of the smaller dimension after the resizing operation.\n",
    "    \n",
    "    Returns:\n",
    "    -img3: 4-dimensional image tensor. shape is [H,W,C,1]\n",
    "\"\"\"\n",
    "function preprocess(img; new_size=256)\n",
    "    height = size(img,1); width = size(img,2);\n",
    "    #Resize so that larger dimension size is equal to the new_size. Note that the height/width ratio is preserved.\n",
    "    if max(height, width) == new_size\n",
    "        img2 = img\n",
    "    elseif height >= width\n",
    "        img2 = imresize(img, new_size, Int(round(new_size*width/height)))\n",
    "    else\n",
    "        img2 = imresize(img, Int(round(new_size*height/width)), new_size)\n",
    "    end\n",
    "    #convert the image to 3D Tensor\n",
    "    img2 = channelview(img2)          #shape is [C,H,W]\n",
    "    img2 = permutedims(img2, [2,3,1]) #shape is [H,W,C]\n",
    "    img2 = Array{Float64}(img2);\n",
    "    #Normalize the image tensor by subtracting the mean of images on which the network(VGG16) has been trained.\n",
    "    MODEL_MEAN = Array{Float64}(averageImage./255)  #squeze to 0-1 interval\n",
    "    img_normalize!(img2, MODEL_MEAN)   \n",
    "    #append an extra dimension at the end of the tensor. \n",
    "    img3 = reshape(img2, (size(img2)..., 1))  #shape is [H,W,C,1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'postprocess :: Tuple{Any}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "postprocess"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    "    Converts an image tensor into image object stored in CPU.\n",
    "    Input:\n",
    "    -img:  4-dimensional image tensor. shape is [H,W,C,1]\n",
    "\n",
    "    Returns:\n",
    "    -img2: image object (to be displayed or saved)\n",
    "\"\"\"\n",
    "function postprocess(img)\n",
    "    img = reshape(img, (size(img)...)[1:end-1])      #shape is [H,W,C]\n",
    "    #Denormalize the image tensor by adding the network mean\n",
    "    MODEL_MEAN = Array{Float64}(averageImage./255)\n",
    "    img_normalize!(img, -1*MODEL_MEAN)\n",
    "    #clamp the tensor so that it becomes a valid image object\n",
    "    clamp!(img, 0,1)\n",
    "    img = Array{FixedPointNumbers.Normed{UInt8,8}}(img)\n",
    "    img2 = colorview(RGB, permutedims(img, [3,1,2])) #conver to RGB image object. shape is [C,H,W]\n",
    "    cpu_type(img2) #transfer the image object to CPU.\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "display_output (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##!!!DELETE THIS\n",
    "#IF NOT NEEDED\n",
    "#Converts an image tensor into an image object inside the CPU.\n",
    "function display_output(img)\n",
    "    output = postprocess(cpu_type(copy(img)))\n",
    "    return output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Knet.KnetArray{Float64,N} where N"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the type functions\n",
    "cpu_type = Array{Float64}\n",
    "if gpu()>-1\n",
    "    dtype = KnetArray{Float64}   #use KnetArray for GPU support, if there is a connected GPU.\n",
    "else\n",
    "    dtype = Array{Float64}   #use CPU array if no GPU is available.\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant LAYER_TYPES\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Load the parameters of the pre-trained network (VGG16)\n",
    "    Inputs:\n",
    "    -CNN: a pre-trained model\n",
    "    -atype: the type to which the network weights will be converted.\n",
    "    -last_layer: the last layer that will be possibly used used for feature representation.\n",
    "    \n",
    "    Returns:\n",
    "    -CNN parameters i.e. (weights, operations, derivatives) for all layers until last_layer\n",
    "\"\"\"\n",
    "function get_params(CNN, atype; last_layer=\"pool5\")\n",
    "    layers = CNN[\"layers\"]\n",
    "    weights, operations, derivatives = [], [], []\n",
    "    for l in layers\n",
    "        get_layer_type(x) = startswith(l[\"name\"], x)\n",
    "        last_layer != nothing && get_layer_type(last_layer) && break\n",
    "        operation = filter(x -> get_layer_type(x), LAYER_TYPES)[1]\n",
    "        push!(operations, operation)\n",
    "        push!(derivatives, haskey(l, \"weights\") && length(l[\"weights\"]) != 0)\n",
    "        if derivatives[end]\n",
    "            w = copy(l[\"weights\"])\n",
    "            if operation == \"conv\"\n",
    "                w[2] = reshape(w[2], (1,1,length(w[2]),1))\n",
    "            elseif operation == \"fc\"\n",
    "                w[1] = transpose(mat(w[1]))\n",
    "            end\n",
    "            push!(weights, w)\n",
    "        end\n",
    "    end\n",
    "    map(w -> map(wi->convert(atype,wi), w), weights), operations, derivatives\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Compute the feature representations for an input image.\n",
    "    Inputs of get_convnet:\n",
    "    -(weights, operations, derivatives) : Parameters of the network which were obtained using get_params function.\n",
    "    Inputs of convnet:\n",
    "    -img_feat: The input iamge that is fed to the network\n",
    "    \n",
    "    Returns:\n",
    "    -outputs: Feature representations obtained from conv and relu layers until the last layer\n",
    "              i.e. (conv1_1, relu1_1, con1_2, relu1_2, con2_1, relu2_1,..., relui_j) where \n",
    "              i is the last layer and j is the last sublayer\n",
    "\"\"\"\n",
    "function get_convnet(weights, operations, derivatives)\n",
    "    function convnet(img_feat)     #img_feat is the input image to the network\n",
    "        outputs = []\n",
    "        i, j = 1, 1\n",
    "        num_weights, num_operations = length(weights), length(operations)\n",
    "        while i <= num_operations && j <= num_weights\n",
    "            if derivatives[i]\n",
    "                img_feat = forw(img_feat, operations[i], weights[j])\n",
    "                j += 1\n",
    "            else\n",
    "                img_feat = forw(img_feat, operations[i])\n",
    "            end\n",
    "            if operations[i] in (\"conv\", \"relu\")\n",
    "                push!(outputs, img_feat)\n",
    "            end\n",
    "            i += 1\n",
    "        end\n",
    "        return outputs\n",
    "    end\n",
    "end\n",
    "\n",
    "#\n",
    "# Define the convolutional network operations, parameters, and feature generator function.\n",
    "convx(x,w) = conv4(w[1], x; padding=1, mode=1) .+ w[2]\n",
    "if VERSION >= v\"0.6.0\"\n",
    "    relux(x) = relu.(x)\n",
    "else\n",
    "    relux(x) = relu(x)\n",
    "end\n",
    "#poolx = pool   #max pooling\n",
    "poolx(x) = pool(x, mode=1)  #avg pooling. This yields better results than max pooling according to the paper.\n",
    "probx(x) = x\n",
    "fcx(x,w) = w[1] * mat(x) .+ w[2]\n",
    "tofunc(op) = eval(parse(string(op, \"x\")))\n",
    "forw(x,op) = tofunc(op)(x)\n",
    "forw(x,op,w) = tofunc(op)(x,w)\n",
    "\n",
    "const LAYER_TYPES = [\"conv\", \"relu\", \"pool\", \"fc\", \"prob\"] \n",
    "global _vggcache\n",
    "VGG_model = \"imagenet-vgg-verydeep-16\"\n",
    "if !isdefined(:_vggcache); _vggcache=Dict(); end\n",
    "if !haskey(_vggcache, VGG_model)\n",
    "    vgg = matconvnet(VGG_model)      #Load the pre-trained model(VGG-16)\n",
    "    params = get_params(vgg, dtype)  #Get the parameters of the model\n",
    "    convnet = get_convnet(params...) #Construct the forward feature generator function using the weights.\n",
    "    #averageImage:mean of the images on which the network has been trained on (VGG16 -> imagenet dataset)s\n",
    "    averageImage = reshape(convert(Array{Float64},vgg[\"meta\"][\"normalization\"][\"averageImage\"]), (1,3))\n",
    "    _vggcache[VGG_model] = vgg, params, convnet, averageImage;\n",
    "else\n",
    "    vgg, params, convnet, averageImage = _vggcache[VGG_model];\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Loss Computation\n",
    "The paper defines the total loss as the weighted average of two loss components, namely content and style loss. Content loss is defined as the sum of squared error between the features of content image and generated image, whereas the style loss is the sum of squared error between the features of style image and generated image. This notebook, however, uses another additional loss component. This 3rd loss is called total variation loss and its aim is to make the resulting image smoother. To compute this loss componet, the generated image matrix is shifted in vertical and horizontal directions by 1 index to obtain two additional shifted versions. Then the sum of squared error is computed between the original and shifted versions of the generated image pairwise and the sum of these two losses yield the total variation loss. Finally, the overall loss is the weighted average of these 3 losses.\n",
    "## Content Loss\n",
    "The content features are obtained by directly taking the representation i.e. the filter repsonses for the input at a particular layer in the network. Assuming that the content features are extracted at the end of layer ${\\ell}$, the content features of the content image is denoted by $P_{ij}^{\\ell}$ and content features of the generated image is denoted by $F_{ij}^{\\ell}$. ${\\frac25 \\pi}$\n",
    "Then, the Content Loss is calculated as following:\n",
    "\n",
    "$L_{content} = \\frac{w_{content}}{} \\times \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$\n",
    "## Style Loss\n",
    "The style loss calculation is a little bit more involved than content loss calculation. First of all, multiple layers are used for style feature calculations unlike the content feature case where just one layer is used. The representations are taken at the set of fixed layers by computing the filter responses fired at that layers when the input is passed through the network. Typically, each layer yields a representation matrix for an image. Then, we compute the Gram Matrix on each layer using the feature representation at that layer. The Gram Matrix is a symmetric matrix and calculated using by taking the dot products of responses of different filters in a layer. In other words, it stores the feature correlations between different filters on the same layer. The Gram Matrix is calculated for a set of layers and the resulting set of matrices is called the style representation of an image. This procedure is performed for the style image and generated image separately. At each layer, an error term is computed using the Gram matrices obtained for style image and generated image. Then, the weighted average of these error terms yields the Style Loss.\n",
    "Let $G_{ij}^\\ell$ is denoting the Gram Matrix for the generated image at layer $\\ell$. It is calculated as following:\n",
    "$$G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}$$\n",
    "Let also , $A^\\ell_{ij}$ denotes the Gram Matrix for the style image at layer $\\ell$, which is calculated similarly.\n",
    "\n",
    "Then, the Style Loss is calculated as following:\n",
    "$$L_s^\\ell = w_\\ell \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Computation\n",
    "The paper defines the total loss as the weighted average of two loss components, namely content and style loss. Content loss is defined as the sum of squared error between the features of content image and generated image, whereas the style loss is the sum of squared error between the features of style image and generated image.\n",
    "This notebook, however, uses another additional loss component. This 3rd loss is called total variation loss and its aim is to make the resulting image smoother. To compute this loss componet, the generated image matrix is shifted in vertical and horizontal directions by 1 index to obtain two additional shifted versions. Then the sum of squared error is computed between the original and shifted versions of the generated image pairwise and the sum of these two losses yield the total variation loss. Finally, the overall loss is the weighted average of these 3 losses.\n",
    "\n",
    "\n",
    "$L_{content} = w_{content} \\times \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Compute \n",
    "    Inputs:\n",
    "    -CNN: a pre-trained model\n",
    "    -atype: the type to which the network weights will be converted.\n",
    "    -last_layer: the last layer that will be possibly used used for feature representation.\n",
    "    \n",
    "    Returns:\n",
    "    -CNN parameters i.e. (weights, operations, derivatives) for all layers until last_layer\n",
    "\"\"\"\n",
    "function content_loss(content_weight, content_current, content_original)\n",
    "    #=\n",
    "    inputs:\n",
    "    compute the content loss from the content image and generated image\n",
    "    content_weight: weight of the content_loss in the total loss function\n",
    "    content_current: features of the current image. shape is (H_l, W_l, C_l, 1)\n",
    "    content_target: features of the content image. shape is (H_l, W_l, C_l ,1)\n",
    "    returning:\n",
    "    content_loss: a scalar loss value\n",
    "    =#\n",
    "    Hl, Wl, Cl, Q = size(content_current)\n",
    "    content_losses = content_weight * sum((content_current - content_original).^2) /  (4*Hl*Wl* Cl)\n",
    "    return content_losses\n",
    "end\n",
    "\n",
    "function gram_matrix(features, normalize=true)\n",
    "    #=\n",
    "    compute the Gram matrix for one layer of network from features\n",
    "    features:input image features. shape (H, W, C, N) where N is 1.\n",
    "    normalize: if true, normalize the Gram matrix by dividing by the number of parameters (H*W*C)\n",
    "    gram_mat: Gram matrix for the input image features. shape (C, C)\n",
    "    =#\n",
    "    H, W, C, N = size(features)\n",
    "    feat_reshaped = reshape(features, (H*W, C))  \n",
    "    gram_mat = transpose(feat_reshaped) * feat_reshaped  #shape:(C,C)\n",
    "    if normalize\n",
    "        return gram_mat ./ (2*H*W*C)\n",
    "    else\n",
    "        return gram_mat\n",
    "    end\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the style loss at a given set of layers\n",
    "Inputs:\n",
    " - feats: List of features obtained at specifed layers for the current image, as obtained by get_convnet function.\n",
    " - style_layers: List of layer indices from which the style will be extracted\n",
    " - style_targets: List of Gram matrices computed from the features yielded by the specified style layers\n",
    " - style_weights: List of weights for the style layers\n",
    "Returning:\n",
    " - style_loss: The scalar style loss obtained by taking the weighted sum of style losses in specified layers\n",
    "\"\"\"\n",
    "function style_loss(feats, style_layers, style_targets, style_weights)\n",
    "    style_losses = Float64(0.0)\n",
    "    for i in 1:length(style_layers)\n",
    "        gram_mat = gram_matrix(feats[style_layers[i]])\n",
    "        style_losses = style_losses + style_weights[i]*sum((gram_mat - style_targets[i]).^2)\n",
    "    end\n",
    "    return style_losses\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute total variation loss.\n",
    "    \n",
    "Inputs:\n",
    "- img: input image, size (H, W, 3, 1)\n",
    "- tv_weight: weight of the TV loss in the overall loss equation\n",
    "    \n",
    "Returning:\n",
    "- losses: total variation loss multiplied by its weight in the overall loss equation\n",
    "\"\"\"\n",
    "\n",
    "#Total-variation regularization\n",
    "function tv_loss(img, tv_weight)\n",
    "    img = Array(img)\n",
    "    Hl, Wl, Cl, Q = size(img)\n",
    "    ver_comp = sum((img[2:end, :, :, :] - img[1:end-1, :, :, :]).^2)\n",
    "    hor_comp = sum((img[:, 2:end, :, :] - img[:, 1:end-1, :, :]).^2)\n",
    "    losses = tv_weight .* (ver_comp + hor_comp) /  (4*Hl*Wl* Cl)\n",
    "    return losses\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Overall loss function that we want to optimize, by updating img_var\n",
    "function loss(img_var, content_weight,content_layer,content_target, style_layers, style_targets, style_weights, tv_weight)\n",
    "    feats = convnet(img_var)\n",
    "    # Compute loss    \n",
    "    c_loss = content_loss(content_weight, feats[content_layer], content_target)\n",
    "    s_loss = style_loss(feats, style_layers, style_targets, style_weights)\n",
    "    t_loss = tv_loss(img_var, tv_weight)\n",
    "    total_loss = c_loss + s_loss + t_loss\n",
    "    return total_loss\n",
    "end\n",
    "\n",
    "loss_gradient = gradloss(loss)  #autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "    Run the neural style transfer algorithm.\n",
    "\n",
    "    Inputs:\n",
    "    - content_image: filename of content image\n",
    "    - style_image: filename of style image\n",
    "    - image_size: size of smallest image dimension (used for content loss and generated image)\n",
    "    - style_size: size of smallest style image dimension\n",
    "    - content_layer: layer to compute the content representation\n",
    "    - content_weight: weight of the content loss\n",
    "    - style_layers: layers to compute the style representations\n",
    "    - style_weights: weights of style layers in the loss function\n",
    "    - tv_weight: weight of the total variation loss, which is used for smoothing\n",
    "    - init_random: if true, initialize to random noise\n",
    " \"\"\"\n",
    "iterations = 500\n",
    "\n",
    "function style_transfer(content_image, style_image, image_size, style_size, content_layer, content_weight,\n",
    "                   style_layers, style_weights, tv_weight, init_random = false)\n",
    "    # Extract features for the content image\n",
    "    content_img = preprocess(load(content_image), new_size=image_size)\n",
    "    content_img_var = dtype(content_img)\n",
    "    feats = convnet(content_img_var)\n",
    "    content_target = copy(feats[content_layer])\n",
    "    \n",
    "    # Extract features for the style image\n",
    "    style_img = preprocess(load(style_image), new_size=style_size)\n",
    "    style_img_var = dtype(style_img)\n",
    "    feats = convnet(style_img_var)\n",
    "    style_targets = []\n",
    "    for i in style_layers\n",
    "        push!(style_targets, gram_matrix(copy(feats[i])))\n",
    "    end\n",
    "    # Note that we are optimizing the pixel values of the image\n",
    "    # Initialize output image to content image or noise\n",
    "    if init_random\n",
    "        #img_var = dtype(rand(size(content_img)))   #uniform r.v. between 0 and 1\n",
    "        img_var = dtype(randn(size(content_img)))  #gaussian noise (mean:0, var:1)\n",
    "    else\n",
    "        img_var = dtype(copy(content_img))\n",
    "    end\n",
    "    \n",
    "    # Set up optimization hyperparameters\n",
    "    initial_lr = 0.03   #0.03 is good for size:256.and 0.01 is good for 512.    \n",
    "    BETA_1=0.9\n",
    "    BETA_2=0.999\n",
    "    EPS=1e-08\n",
    "    #the img_var will be updated using Adam\n",
    "    optim = optimizers(img_var, Adam; lr=initial_lr, beta1=BETA_1, beta2=BETA_2, eps=EPS)\n",
    "    \n",
    "    #TRAINING: TAKE DERIVATIVE W.R.T. img_var and update img_var\n",
    "    #initialize the loss_vector so that we can plot the loss in each iteration later\n",
    "    loss_vector = []\n",
    "    info(\"Training...\")\n",
    "    for t in 1:iterations\n",
    "        grads, loss_value = loss_gradient(img_var, content_weight,content_layer,content_target, \n",
    "            style_layers, style_targets, style_weights, tv_weight)\n",
    "        \n",
    "        update!(img_var, grads, optim)\n",
    "        \n",
    "        #report errors in every five iterations\n",
    "        if t % 50 == 0\n",
    "                println((:iteration, t, :loss, loss_value))\n",
    "        end\n",
    "        push!(loss_vector, loss_value)\n",
    "    end\n",
    "    return (img_var, loss_vector);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Demo-1. Style transfer.\n",
    "content_img_def = \"content_images/sculpture1.jpg\"\n",
    "style_img_def = \"style_images/style3_buddha.jpg\"\n",
    "iterations = 500\n",
    "image_size_default = style_size_default = 256\n",
    "content_lay_default = 18    #relu4_2\n",
    "style_lay_default = (3, 7, 12, 17, 23);  #conv1_2, conv2_2, conv3_2, conv4_2, conv5_2\n",
    "\n",
    "content_weights_default = 10*Float64(1.0)\n",
    "style_weights_default = 10*Array{Float64}([0.2, 2*0.2, 3*0.2, 4*0.2, 5*0.2])\n",
    "tv_weight_def = 8*Float64(1.0);\n",
    "@time (img1, loss_vector1) = style_transfer(content_img_def, style_img_def, image_size_default, style_size_default, content_lay_default, \n",
    "    content_weights_default, style_lay_default, style_weights_default, tv_weight_def);\n",
    "#save(\"output_image.jpg\", output1)\n",
    "output1=postprocess(img1);\n",
    "display(output1)\n",
    "#output1 = display_output(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
